{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d3a118-c91b-4c2f-bce7-8307b379b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for Random Forest: 0.9795501022494888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [04:53:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for XGBoost: 0.983640081799591\n",
      "Validation Accuracy for Logistic Regression: 0.9815950920245399\n",
      "Test predictions saved to pred_combined.txt\n",
      "Number of parameters in the Logistic Regression model: 101\n",
      "Accuracy: 98.57%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       252\n",
      "           1       0.98      0.99      0.99       237\n",
      "\n",
      "    accuracy                           0.99       489\n",
      "   macro avg       0.99      0.99      0.99       489\n",
      "weighted avg       0.99      0.99      0.99       489\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Accuracy: 92.84%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       252\n",
      "           1       0.92      0.93      0.93       237\n",
      "\n",
      "    accuracy                           0.93       489\n",
      "   macro avg       0.93      0.93      0.93       489\n",
      "weighted avg       0.93      0.93      0.93       489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.5232 - loss: 0.6919 - val_accuracy: 0.5910 - val_loss: 0.6752\n",
      "Epoch 2/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.5957 - loss: 0.6652 - val_accuracy: 0.6360 - val_loss: 0.6439\n",
      "Epoch 3/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.6041 - loss: 0.6522 - val_accuracy: 0.6360 - val_loss: 0.6430\n",
      "Epoch 4/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6106 - loss: 0.6516 - val_accuracy: 0.6299 - val_loss: 0.6408\n",
      "Epoch 5/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6144 - loss: 0.6411 - val_accuracy: 0.6196 - val_loss: 0.6462\n",
      "Epoch 6/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6311 - loss: 0.6440 - val_accuracy: 0.6360 - val_loss: 0.6416\n",
      "Epoch 7/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6198 - loss: 0.6467 - val_accuracy: 0.6462 - val_loss: 0.6412\n",
      "Epoch 8/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6253 - loss: 0.6459 - val_accuracy: 0.6503 - val_loss: 0.6298\n",
      "Epoch 9/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.6358 - loss: 0.6379 - val_accuracy: 0.6299 - val_loss: 0.6271\n",
      "Epoch 10/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6271 - loss: 0.6345 - val_accuracy: 0.6258 - val_loss: 0.6289\n",
      "Epoch 11/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6272 - loss: 0.6242 - val_accuracy: 0.6728 - val_loss: 0.6185\n",
      "Epoch 12/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.6402 - loss: 0.6201 - val_accuracy: 0.6442 - val_loss: 0.6194\n",
      "Epoch 13/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6489 - loss: 0.6214 - val_accuracy: 0.6421 - val_loss: 0.6216\n",
      "Epoch 14/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.6558 - loss: 0.6132 - val_accuracy: 0.6503 - val_loss: 0.6151\n",
      "Epoch 15/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6427 - loss: 0.6243 - val_accuracy: 0.6585 - val_loss: 0.6122\n",
      "Epoch 16/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.6602 - loss: 0.6142 - val_accuracy: 0.6687 - val_loss: 0.6076\n",
      "Epoch 17/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.6602 - loss: 0.6135 - val_accuracy: 0.7035 - val_loss: 0.6046\n",
      "Epoch 18/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6648 - loss: 0.6027 - val_accuracy: 0.6892 - val_loss: 0.6125\n",
      "Epoch 19/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6698 - loss: 0.6024 - val_accuracy: 0.7076 - val_loss: 0.5984\n",
      "Epoch 20/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6798 - loss: 0.5939 - val_accuracy: 0.6871 - val_loss: 0.5902\n",
      "Epoch 21/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6841 - loss: 0.5881 - val_accuracy: 0.6851 - val_loss: 0.5946\n",
      "Epoch 22/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6858 - loss: 0.5920 - val_accuracy: 0.6973 - val_loss: 0.5848\n",
      "Epoch 23/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6912 - loss: 0.5782 - val_accuracy: 0.6912 - val_loss: 0.5870\n",
      "Epoch 24/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6902 - loss: 0.5789 - val_accuracy: 0.6892 - val_loss: 0.5869\n",
      "Epoch 25/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.6931 - loss: 0.5793 - val_accuracy: 0.6953 - val_loss: 0.5780\n",
      "Epoch 26/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7018 - loss: 0.5691 - val_accuracy: 0.6953 - val_loss: 0.5757\n",
      "Epoch 27/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7018 - loss: 0.5636 - val_accuracy: 0.6851 - val_loss: 0.5854\n",
      "Epoch 28/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7138 - loss: 0.5608 - val_accuracy: 0.7137 - val_loss: 0.5719\n",
      "Epoch 29/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.7030 - loss: 0.5670 - val_accuracy: 0.6933 - val_loss: 0.5638\n",
      "Epoch 30/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.7070 - loss: 0.5580 - val_accuracy: 0.7014 - val_loss: 0.5649\n",
      "Epoch 31/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7172 - loss: 0.5525 - val_accuracy: 0.6994 - val_loss: 0.5648\n",
      "Epoch 32/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.7308 - loss: 0.5466 - val_accuracy: 0.7055 - val_loss: 0.5583\n",
      "Epoch 33/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7304 - loss: 0.5362 - val_accuracy: 0.7035 - val_loss: 0.5559\n",
      "Epoch 34/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7301 - loss: 0.5325 - val_accuracy: 0.7280 - val_loss: 0.5454\n",
      "Epoch 35/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7236 - loss: 0.5359 - val_accuracy: 0.7014 - val_loss: 0.5484\n",
      "Epoch 36/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7311 - loss: 0.5311 - val_accuracy: 0.7137 - val_loss: 0.5385\n",
      "Epoch 37/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.7447 - loss: 0.5080 - val_accuracy: 0.7301 - val_loss: 0.5389\n",
      "Epoch 38/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7390 - loss: 0.5161 - val_accuracy: 0.7055 - val_loss: 0.5497\n",
      "Epoch 39/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7532 - loss: 0.5086 - val_accuracy: 0.7403 - val_loss: 0.5184\n",
      "Epoch 40/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7405 - loss: 0.5087 - val_accuracy: 0.7423 - val_loss: 0.5201\n",
      "Epoch 41/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7630 - loss: 0.4847 - val_accuracy: 0.7362 - val_loss: 0.5136\n",
      "Epoch 42/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7648 - loss: 0.4859 - val_accuracy: 0.7403 - val_loss: 0.5097\n",
      "Epoch 43/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7535 - loss: 0.4877 - val_accuracy: 0.7219 - val_loss: 0.5486\n",
      "Epoch 44/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7788 - loss: 0.4609 - val_accuracy: 0.7546 - val_loss: 0.5002\n",
      "Epoch 45/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.7824 - loss: 0.4534 - val_accuracy: 0.7423 - val_loss: 0.5123\n",
      "Epoch 46/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.7907 - loss: 0.4390 - val_accuracy: 0.7505 - val_loss: 0.4979\n",
      "Epoch 47/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8051 - loss: 0.4242 - val_accuracy: 0.7751 - val_loss: 0.4825\n",
      "Epoch 48/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.7933 - loss: 0.4439 - val_accuracy: 0.7628 - val_loss: 0.4946\n",
      "Epoch 49/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8031 - loss: 0.4244 - val_accuracy: 0.7710 - val_loss: 0.4768\n",
      "Epoch 50/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8127 - loss: 0.4061 - val_accuracy: 0.7607 - val_loss: 0.5034\n",
      "Epoch 51/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8103 - loss: 0.4014 - val_accuracy: 0.7751 - val_loss: 0.4718\n",
      "Epoch 52/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8091 - loss: 0.4120 - val_accuracy: 0.7791 - val_loss: 0.4719\n",
      "Epoch 53/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8249 - loss: 0.3878 - val_accuracy: 0.7812 - val_loss: 0.4539\n",
      "Epoch 54/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8262 - loss: 0.3823 - val_accuracy: 0.7853 - val_loss: 0.4400\n",
      "Epoch 55/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8348 - loss: 0.3617 - val_accuracy: 0.7853 - val_loss: 0.4429\n",
      "Epoch 56/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8503 - loss: 0.3559 - val_accuracy: 0.8016 - val_loss: 0.4418\n",
      "Epoch 57/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8386 - loss: 0.3570 - val_accuracy: 0.7914 - val_loss: 0.4394\n",
      "Epoch 58/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8459 - loss: 0.3465 - val_accuracy: 0.7955 - val_loss: 0.4381\n",
      "Epoch 59/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8485 - loss: 0.3441 - val_accuracy: 0.7975 - val_loss: 0.4182\n",
      "Epoch 60/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8548 - loss: 0.3350 - val_accuracy: 0.8098 - val_loss: 0.4118\n",
      "Epoch 61/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8620 - loss: 0.3228 - val_accuracy: 0.7975 - val_loss: 0.4602\n",
      "Epoch 62/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8510 - loss: 0.3315 - val_accuracy: 0.8282 - val_loss: 0.4066\n",
      "Epoch 63/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8563 - loss: 0.3158 - val_accuracy: 0.7975 - val_loss: 0.4191\n",
      "Epoch 64/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8570 - loss: 0.3188 - val_accuracy: 0.8037 - val_loss: 0.3939\n",
      "Epoch 65/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8619 - loss: 0.3153 - val_accuracy: 0.8262 - val_loss: 0.4100\n",
      "Epoch 66/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8807 - loss: 0.2795 - val_accuracy: 0.8241 - val_loss: 0.4080\n",
      "Epoch 67/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8765 - loss: 0.2824 - val_accuracy: 0.8160 - val_loss: 0.4079\n",
      "Epoch 68/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8767 - loss: 0.2808 - val_accuracy: 0.8344 - val_loss: 0.3854\n",
      "Epoch 69/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8764 - loss: 0.2835 - val_accuracy: 0.8425 - val_loss: 0.3924\n",
      "Epoch 70/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8897 - loss: 0.2623 - val_accuracy: 0.8262 - val_loss: 0.3859\n",
      "Epoch 71/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8844 - loss: 0.2751 - val_accuracy: 0.8344 - val_loss: 0.3935\n",
      "Epoch 72/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8894 - loss: 0.2590 - val_accuracy: 0.8344 - val_loss: 0.4111\n",
      "Epoch 73/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8904 - loss: 0.2547 - val_accuracy: 0.8466 - val_loss: 0.3818\n",
      "Epoch 74/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8940 - loss: 0.2495 - val_accuracy: 0.8446 - val_loss: 0.3960\n",
      "Epoch 75/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8888 - loss: 0.2548 - val_accuracy: 0.8303 - val_loss: 0.4011\n",
      "Epoch 76/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8863 - loss: 0.2655 - val_accuracy: 0.8446 - val_loss: 0.3939\n",
      "Epoch 77/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9086 - loss: 0.2272 - val_accuracy: 0.8303 - val_loss: 0.3842\n",
      "Epoch 78/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8993 - loss: 0.2350 - val_accuracy: 0.8323 - val_loss: 0.4151\n",
      "Epoch 79/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9018 - loss: 0.2342 - val_accuracy: 0.8466 - val_loss: 0.3780\n",
      "Epoch 80/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9068 - loss: 0.2162 - val_accuracy: 0.8364 - val_loss: 0.3878\n",
      "Epoch 81/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9057 - loss: 0.2303 - val_accuracy: 0.8507 - val_loss: 0.3952\n",
      "Epoch 82/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9184 - loss: 0.2045 - val_accuracy: 0.8364 - val_loss: 0.3977\n",
      "Epoch 83/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9152 - loss: 0.2042 - val_accuracy: 0.8323 - val_loss: 0.4074\n",
      "Epoch 84/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9211 - loss: 0.2021 - val_accuracy: 0.8528 - val_loss: 0.3808\n",
      "Epoch 85/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9173 - loss: 0.2028 - val_accuracy: 0.8630 - val_loss: 0.4011\n",
      "Epoch 86/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9101 - loss: 0.2100 - val_accuracy: 0.8528 - val_loss: 0.3725\n",
      "Epoch 87/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9218 - loss: 0.1893 - val_accuracy: 0.8507 - val_loss: 0.4050\n",
      "Epoch 88/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9300 - loss: 0.1799 - val_accuracy: 0.8589 - val_loss: 0.4067\n",
      "Epoch 89/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9245 - loss: 0.1943 - val_accuracy: 0.8405 - val_loss: 0.4158\n",
      "Epoch 90/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9257 - loss: 0.1808 - val_accuracy: 0.8262 - val_loss: 0.4188\n",
      "Epoch 91/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9289 - loss: 0.1805 - val_accuracy: 0.8303 - val_loss: 0.4267\n",
      "Epoch 92/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9276 - loss: 0.1833 - val_accuracy: 0.8446 - val_loss: 0.4143\n",
      "Epoch 93/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9323 - loss: 0.1710 - val_accuracy: 0.8446 - val_loss: 0.4235\n",
      "Epoch 94/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9322 - loss: 0.1731 - val_accuracy: 0.8548 - val_loss: 0.4092\n",
      "Epoch 95/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9358 - loss: 0.1609 - val_accuracy: 0.8466 - val_loss: 0.4320\n",
      "Epoch 96/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9361 - loss: 0.1582 - val_accuracy: 0.8528 - val_loss: 0.4161\n",
      "Epoch 97/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9418 - loss: 0.1450 - val_accuracy: 0.8446 - val_loss: 0.4435\n",
      "Epoch 98/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9390 - loss: 0.1545 - val_accuracy: 0.8507 - val_loss: 0.4187\n",
      "Epoch 99/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9330 - loss: 0.1651 - val_accuracy: 0.8589 - val_loss: 0.4383\n",
      "Epoch 100/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9427 - loss: 0.1544 - val_accuracy: 0.8589 - val_loss: 0.4295\n",
      "Epoch 101/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9371 - loss: 0.1518 - val_accuracy: 0.8528 - val_loss: 0.4539\n",
      "Epoch 102/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9358 - loss: 0.1630 - val_accuracy: 0.8589 - val_loss: 0.3887\n",
      "Epoch 103/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9418 - loss: 0.1525 - val_accuracy: 0.8569 - val_loss: 0.4511\n",
      "Epoch 104/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9464 - loss: 0.1455 - val_accuracy: 0.8528 - val_loss: 0.4472\n",
      "Epoch 105/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9514 - loss: 0.1319 - val_accuracy: 0.8569 - val_loss: 0.4589\n",
      "Epoch 106/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9512 - loss: 0.1260 - val_accuracy: 0.8446 - val_loss: 0.5036\n",
      "Epoch 107/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9479 - loss: 0.1328 - val_accuracy: 0.8446 - val_loss: 0.4920\n",
      "Epoch 108/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9444 - loss: 0.1393 - val_accuracy: 0.8487 - val_loss: 0.4585\n",
      "Epoch 109/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9536 - loss: 0.1211 - val_accuracy: 0.8528 - val_loss: 0.4858\n",
      "Epoch 110/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9528 - loss: 0.1299 - val_accuracy: 0.8528 - val_loss: 0.5066\n",
      "Epoch 111/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9539 - loss: 0.1265 - val_accuracy: 0.8528 - val_loss: 0.4926\n",
      "Epoch 112/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9568 - loss: 0.1201 - val_accuracy: 0.8528 - val_loss: 0.5283\n",
      "Epoch 113/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9504 - loss: 0.1258 - val_accuracy: 0.8630 - val_loss: 0.4920\n",
      "Epoch 114/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9548 - loss: 0.1136 - val_accuracy: 0.8507 - val_loss: 0.5279\n",
      "Epoch 115/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9542 - loss: 0.1249 - val_accuracy: 0.8507 - val_loss: 0.5006\n",
      "Epoch 116/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9119 - loss: 0.2500 - val_accuracy: 0.8507 - val_loss: 0.4836\n",
      "Epoch 117/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9626 - loss: 0.1089 - val_accuracy: 0.8487 - val_loss: 0.5147\n",
      "Epoch 118/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9589 - loss: 0.1099 - val_accuracy: 0.8569 - val_loss: 0.5167\n",
      "Epoch 119/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9690 - loss: 0.0987 - val_accuracy: 0.8425 - val_loss: 0.5272\n",
      "Epoch 120/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9581 - loss: 0.1138 - val_accuracy: 0.8569 - val_loss: 0.5359\n",
      "Epoch 121/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9589 - loss: 0.1083 - val_accuracy: 0.8344 - val_loss: 0.5646\n",
      "Epoch 122/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9606 - loss: 0.1137 - val_accuracy: 0.8630 - val_loss: 0.5210\n",
      "Epoch 123/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9674 - loss: 0.0983 - val_accuracy: 0.8344 - val_loss: 0.5490\n",
      "Epoch 124/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9579 - loss: 0.1207 - val_accuracy: 0.8487 - val_loss: 0.5449\n",
      "Epoch 125/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9599 - loss: 0.1016 - val_accuracy: 0.8507 - val_loss: 0.5622\n",
      "Epoch 126/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9712 - loss: 0.0902 - val_accuracy: 0.8425 - val_loss: 0.5452\n",
      "Epoch 127/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9688 - loss: 0.0903 - val_accuracy: 0.8507 - val_loss: 0.5594\n",
      "Epoch 128/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9560 - loss: 0.1145 - val_accuracy: 0.8589 - val_loss: 0.5462\n",
      "Epoch 129/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.9638 - loss: 0.1029 - val_accuracy: 0.8487 - val_loss: 0.5543\n",
      "Epoch 130/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9627 - loss: 0.1080 - val_accuracy: 0.8507 - val_loss: 0.5310\n",
      "Epoch 131/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9570 - loss: 0.1110 - val_accuracy: 0.8425 - val_loss: 0.6162\n",
      "Epoch 132/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9712 - loss: 0.0886 - val_accuracy: 0.8548 - val_loss: 0.5731\n",
      "Epoch 133/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9625 - loss: 0.1012 - val_accuracy: 0.8466 - val_loss: 0.5879\n",
      "Epoch 134/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9709 - loss: 0.0839 - val_accuracy: 0.8487 - val_loss: 0.5699\n",
      "Epoch 135/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9677 - loss: 0.0883 - val_accuracy: 0.8569 - val_loss: 0.5875\n",
      "Epoch 136/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9723 - loss: 0.0729 - val_accuracy: 0.8569 - val_loss: 0.6029\n",
      "Epoch 137/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.9710 - loss: 0.0824 - val_accuracy: 0.8446 - val_loss: 0.6081\n",
      "Epoch 138/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9667 - loss: 0.0877 - val_accuracy: 0.8548 - val_loss: 0.5477\n",
      "Epoch 139/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9708 - loss: 0.0821 - val_accuracy: 0.8548 - val_loss: 0.5883\n",
      "Epoch 140/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9755 - loss: 0.0695 - val_accuracy: 0.8589 - val_loss: 0.6356\n",
      "Epoch 141/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9623 - loss: 0.1029 - val_accuracy: 0.8650 - val_loss: 0.5923\n",
      "Epoch 142/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9687 - loss: 0.0931 - val_accuracy: 0.8507 - val_loss: 0.5965\n",
      "Epoch 143/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9647 - loss: 0.0905 - val_accuracy: 0.8466 - val_loss: 0.6297\n",
      "Epoch 144/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9643 - loss: 0.0960 - val_accuracy: 0.8569 - val_loss: 0.6255\n",
      "Epoch 145/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9781 - loss: 0.0713 - val_accuracy: 0.8589 - val_loss: 0.6181\n",
      "Epoch 146/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9724 - loss: 0.0872 - val_accuracy: 0.8548 - val_loss: 0.6336\n",
      "Epoch 147/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9748 - loss: 0.0706 - val_accuracy: 0.8589 - val_loss: 0.5939\n",
      "Epoch 148/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9758 - loss: 0.0677 - val_accuracy: 0.8548 - val_loss: 0.6056\n",
      "Epoch 149/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9765 - loss: 0.0774 - val_accuracy: 0.8507 - val_loss: 0.6417\n",
      "Epoch 150/150\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9596 - loss: 0.1082 - val_accuracy: 0.8425 - val_loss: 0.6566\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "Accuracy: 84.25%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       252\n",
      "           1       0.84      0.83      0.84       237\n",
      "\n",
      "    accuracy                           0.84       489\n",
      "   macro avg       0.84      0.84      0.84       489\n",
      "weighted avg       0.84      0.84      0.84       489\n",
      "\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Test predictions have been saved to respective .txt files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "# Base class for models\n",
    "class MLModel():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Feature-based model (LogisticRegression with PCA for dimensionality reduction)\n",
    "class FeatureModel(MLModel):\n",
    "    def __init__(self):\n",
    "        self.pca = PCA(n_components=100)  # PCA with 100 components\n",
    "        self.model = LogisticRegression(max_iter=100)  # Logistic Regression Model\n",
    "        self.num_parameters = 0  # Placeholder for the number of model parameters\n",
    "\n",
    "    def train(self, X, y):\n",
    "        flattened_features = X.reshape(X.shape[0], -1)\n",
    "        reduced_features = self.pca.fit_transform(flattened_features)\n",
    "        self.model.fit(reduced_features, y)\n",
    "        self.num_parameters = self.model.coef_.size + self.model.intercept_.size\n",
    "        print(f\"Number of parameters in the Logistic Regression model: {self.num_parameters}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        flattened_features = X.reshape(X.shape[0], -1)\n",
    "        reduced_features = self.pca.transform(flattened_features)\n",
    "        return self.model.predict(reduced_features)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "\n",
    "# Emoticon-based model (using LogisticRegression with One-Hot Encoding)\n",
    "class EmoticonModel(MLModel):\n",
    "    def __init__(self):\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.model = LogisticRegression(max_iter=500, penalty='l1', solver='liblinear')\n",
    "        self.best_model = None\n",
    "        self.grid_search = None\n",
    "\n",
    "    def train(self, X, y, train_data_fraction=1.0):\n",
    "        max_length = 13\n",
    "        X_emojis = pd.DataFrame(X.apply(self.split_emojis).tolist(), columns=[f'emoji_{i+1}' for i in range(max_length)])\n",
    "        X_encoded = self.encoder.fit_transform(X_emojis)\n",
    "        if train_data_fraction < 1.0:\n",
    "            X_train_partial, _, y_train_partial, _ = train_test_split(X_encoded, y, train_size=train_data_fraction, random_state=42)\n",
    "        else:\n",
    "            X_train_partial, y_train_partial = X_encoded, y\n",
    "        param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        self.grid_search = GridSearchCV(self.model, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "        self.grid_search.fit(X_train_partial, y_train_partial)\n",
    "        self.best_model = self.grid_search.best_estimator_\n",
    "\n",
    "    def predict(self, X):\n",
    "        max_length = 13\n",
    "        X_emojis = pd.DataFrame(X.apply(self.split_emojis).tolist(), columns=[f'emoji_{i+1}' for i in range(max_length)])\n",
    "        X_encoded = self.encoder.transform(X_emojis)\n",
    "        return self.best_model.predict(X_encoded)\n",
    "\n",
    "    def split_emojis(self, emoji_string):\n",
    "        max_length = 13\n",
    "        emojis = list(emoji_string)\n",
    "        return emojis + [''] * (max_length - len(emojis)) if len(emojis) < max_length else emojis\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "\n",
    "# Text sequence-based model (using LSTM for sequence classification)\n",
    "class TextSeqModel(MLModel):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(char_level=True)\n",
    "        self.max_length = 47\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X, y, X_valid, y_valid):\n",
    "        X = X.apply(lambda x: x[3:])\n",
    "        X_valid = X_valid.apply(lambda x: x[3:])\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        X_train_tokenized = self.tokenizer.texts_to_sequences(X)\n",
    "        X_val_tokenized = self.tokenizer.texts_to_sequences(X_valid)\n",
    "        X_train_padded = pad_sequences(X_train_tokenized, maxlen=self.max_length, padding='post')\n",
    "        X_val_padded = pad_sequences(X_val_tokenized, maxlen=self.max_length, padding='post')\n",
    "        vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        embedding_dim = 16\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=self.max_length))\n",
    "        self.model.add(LSTM(32, return_sequences=False))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(X_train_padded, y, epochs=150, batch_size=32, validation_data=(X_val_padded, y_valid))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.apply(lambda x: x[3:])\n",
    "        X_tokenized = self.tokenizer.texts_to_sequences(X)\n",
    "        X_padded = pad_sequences(X_tokenized, maxlen=self.max_length, padding='post')\n",
    "        return (self.model.predict(X_padded) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "\n",
    "# One-hot encoding function\n",
    "def onehot_encode(train_data, test_data, val_data):\n",
    "    # Fit on the training data and transform both train, validation, and test sets\n",
    "    train_encoded = onehot_encoder.fit_transform(np.array(train_data).reshape(-1, 1)).toarray()\n",
    "    test_encoded = onehot_encoder.transform(np.array(test_data).reshape(-1, 1)).toarray()\n",
    "    val_encoded = onehot_encoder.transform(np.array(val_data).reshape(-1, 1)).toarray()\n",
    "    return train_encoded, test_encoded, val_encoded\n",
    "# Utility function to save predictions to a file\n",
    "def save_predictions_to_file(predictions, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred}\\n\")\n",
    "\n",
    "\n",
    "# Utility function to save predictions to a text file\n",
    "def save_predictions_to_text(predictions, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the datasets\n",
    "    train_emoticon_df = pd.read_csv(\"datasets/train/train_emoticon.csv\")\n",
    "    val_emoticon_df = pd.read_csv(\"datasets/valid/valid_emoticon.csv\")\n",
    "    test_emoticon_df = pd.read_csv(\"datasets/test/test_emoticon.csv\")\n",
    "    \n",
    "    train_seq_df = pd.read_csv(\"datasets/train/train_text_seq.csv\")\n",
    "    val_seq_df = pd.read_csv(\"datasets/valid/valid_text_seq.csv\")\n",
    "    test_seq_df = pd.read_csv(\"datasets/test/test_text_seq.csv\")\n",
    "    \n",
    "    train_feat = np.load(\"datasets/train/train_feature.npz\", allow_pickle=True)\n",
    "    val_feat = np.load(\"datasets/valid/valid_feature.npz\", allow_pickle=True)\n",
    "    test_feat = np.load(\"datasets/test/test_feature.npz\", allow_pickle=True)\n",
    "    \n",
    "    # Prepare input data\n",
    "    train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
    "    val_emoticon_X = val_emoticon_df['input_emoticon'].tolist()\n",
    "    test_emoticon_X = test_emoticon_df['input_emoticon'].tolist()\n",
    "    \n",
    "    train_seq_X = train_seq_df['input_str'].tolist()\n",
    "    val_seq_X = val_seq_df['input_str'].tolist()\n",
    "    test_seq_X = test_seq_df['input_str'].tolist()\n",
    "    \n",
    "    train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
    "    val_emoticon_Y = val_emoticon_df['label'].tolist()\n",
    "    train_feat_X = train_feat['features']\n",
    "    train_feat_Y = train_feat['label']\n",
    "    val_feat_X = val_feat['features']\n",
    "    val_feat_Y = val_feat['label']\n",
    "    \n",
    "    test_feat_X = test_feat['features']\n",
    "    \n",
    "    # Initialize encoders and scalers\n",
    "    scaler = StandardScaler()\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # One-Hot Encode Emoticon Dataset (Training, Validation, and Test)\n",
    "    train_emoticon_encoded, test_emoticon_encoded, val_emoticon_encoded = onehot_encode(train_emoticon_X, test_emoticon_X, val_emoticon_X)\n",
    "    train_seq_encoded, test_seq_encoded, val_seq_encoded = onehot_encode(train_seq_X, test_seq_X, val_seq_X)\n",
    "    \n",
    "    # Scale the feature matrices\n",
    "    train_feat_scaled = scaler.fit_transform(train_feat_X.reshape(train_feat_X.shape[0], -1))\n",
    "    val_feat_scaled = scaler.transform(val_feat_X.reshape(val_feat_X.shape[0], -1))\n",
    "    test_feat_scaled = scaler.transform(test_feat_X.reshape(test_feat_X.shape[0], -1))\n",
    "\n",
    "    # Concatenate all encoded/processed datasets\n",
    "    train_X_combined = np.hstack((train_emoticon_encoded, train_seq_encoded, train_feat_scaled))\n",
    "    val_X_combined = np.hstack((val_emoticon_encoded, val_seq_encoded, val_feat_scaled))\n",
    "    test_X_combined = np.hstack((test_emoticon_encoded, test_seq_encoded, test_feat_scaled))\n",
    "    \n",
    "    # Convert labels to numpy arrays\n",
    "    train_Y_combined = np.array(train_emoticon_Y)\n",
    "    val_Y_combined = np.array(val_emoticon_Y)\n",
    "    \n",
    "    # List of classifiers to test\n",
    "    classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=200, solver='liblinear', random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Open a text file to write the predictions\n",
    "    with open(\"pred_combined.txt\", \"w\") as f:\n",
    "        # Train each classifier and evaluate accuracy\n",
    "        for name, model in classifiers.items():\n",
    "            # Fit the model on training data\n",
    "            model.fit(train_X_combined, train_Y_combined)\n",
    "            \n",
    "            # Make predictions on the validation data\n",
    "            val_pred = model.predict(val_X_combined)\n",
    "            \n",
    "            # Calculate and print accuracy\n",
    "            val_accuracy = accuracy_score(val_Y_combined, val_pred)\n",
    "            print(f\"Validation Accuracy for {name}: {val_accuracy}\")\n",
    "            \n",
    "            # Write validation accuracy to the file\n",
    "            f.write(f\"Validation Accuracy for {name}: {val_accuracy}\\n\")\n",
    "    \n",
    "            # Make predictions on the test data\n",
    "            test_pred = model.predict(test_X_combined)\n",
    "            \n",
    "            # Write the predictions to the file\n",
    "            f.write(f\"Predictions for {name} on Test Data:\\n\")\n",
    "            for pred in test_pred:\n",
    "                f.write(f\"{pred}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(\"Test predictions saved to pred_combined.txt\")\n",
    "\n",
    "    # Load training dataset for FeatureModel\n",
    "    train_data = np.load('datasets/train/train_feature.npz')\n",
    "    train_features = train_data['features']  # Shape: (7080, 13, 768)\n",
    "    train_labels = train_data['label']  # Shape: (7080,)\n",
    "\n",
    "    # Initialize and train FeatureModel\n",
    "    feature_model = FeatureModel()\n",
    "    feature_model.train(train_features, train_labels)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    valid_data = np.load('datasets/valid/valid_feature.npz')\n",
    "    valid_features = valid_data['features']  # Shape: (validation_samples, 13, 768)\n",
    "    valid_labels = valid_data['label']  # Validation labels\n",
    "    feature_model.evaluate(valid_features, valid_labels)\n",
    "\n",
    "    # Load test dataset for FeatureModel\n",
    "    test_data = np.load('datasets/test/test_feature.npz')\n",
    "    test_features = test_data['features']  # Test features\n",
    "    test_predictions_feature = feature_model.predict(test_features)\n",
    "\n",
    "    # Save predictions of FeatureModel to a text file\n",
    "    save_predictions_to_text(test_predictions_feature, 'pred_deepfeat.txt')\n",
    "\n",
    "    # Load training dataset for EmoticonModel\n",
    "    train_emoticon_data = pd.read_csv('datasets/train/train_emoticon.csv')\n",
    "    valid_emoticon_data = pd.read_csv('datasets/valid/valid_emoticon.csv')\n",
    "    test_emoticon_data = pd.read_csv('datasets/test/test_emoticon.csv')\n",
    "\n",
    "    # Target labels for emoticon model\n",
    "    y_train_emoticon = train_emoticon_data['label'].values\n",
    "    y_valid_emoticon = valid_emoticon_data['label'].values\n",
    "\n",
    "    # Create the EmoticonModel instance\n",
    "    emoticon_model = EmoticonModel()\n",
    "    emoticon_model.train(train_emoticon_data['input_emoticon'], y_train_emoticon)\n",
    "    emoticon_model.evaluate(valid_emoticon_data['input_emoticon'], y_valid_emoticon)\n",
    "\n",
    "    # Make predictions on the test data for EmoticonModel\n",
    "    test_predictions_emoticon = emoticon_model.predict(test_emoticon_data['input_emoticon'])\n",
    "\n",
    "    # Save predictions of EmoticonModel to a text file\n",
    "    save_predictions_to_text(test_predictions_emoticon, 'pred_emoticon.txt')\n",
    "\n",
    "    # Load the text sequence dataset for TextSeqModel\n",
    "    train_seq_data = pd.read_csv('datasets/train/train_text_seq.csv')\n",
    "    valid_seq_data = pd.read_csv('datasets/valid/valid_text_seq.csv')\n",
    "    test_seq_data = pd.read_csv('datasets/test/test_text_seq.csv')\n",
    "\n",
    "    # Target labels for text sequence model\n",
    "    y_train_seq = train_seq_data['label'].values\n",
    "    y_valid_seq = valid_seq_data['label'].values\n",
    "\n",
    "    # Create the TextSeqModel instance\n",
    "    text_model = TextSeqModel()\n",
    "    text_model.train(train_seq_data['input_str'], y_train_seq, valid_seq_data['input_str'], y_valid_seq)\n",
    "    text_model.evaluate(valid_seq_data['input_str'], y_valid_seq)\n",
    "\n",
    "    # Make predictions on the test data for TextSeqModel\n",
    "    test_predictions_seq = text_model.predict(test_seq_data['input_str'])\n",
    "\n",
    "    # Save predictions of TextSeqModel to a text file\n",
    "    save_predictions_to_text(test_predictions_seq, 'pred_textseq.txt')\n",
    "\n",
    "    print(\"Test predictions have been saved to pred_textseq.txt files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
